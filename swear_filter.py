import re
import asyncio
from collections import defaultdict
from itertools import product
import time
import unicodedata
from typing import List, Dict, Set, Optional
from functools import lru_cache
from langdetect import detect, LangDetectException
import nltk
from nltk.corpus import wordnet

# ==================== CONSTANTS ====================
HIDDEN_SEPARATORS = [
    '\u200B', '\u200C', '\u200D', '\u2060',  # Zero-width spaces
    '\u034F', '\u180E', '\uFEFF',            # Other hidden chars
    '\u00AD',                                # Soft hyphen
    '\u17B5', '\u17B6',                      # Khmer vowel signs
    '\u2028', '\u2029',                      # Line/paragraph separators
    '\u1160', '\u3164',                      # Hangul filler
]
# Comprehensive character substitution dictionary for text filtering
COMBINED_SUBSTITUTIONS = {
    # Lowercase letters
    'a': ['a', '@', '4', 'Î±', 'Î»', '*', 'â“', 'â’¶', 'ï½', 'ï¼¡', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'á´€', 'É', 'É’', 'Ğ”', 'Ã…', 'ğš', 'ğ’‚', 'ğ’¶', 'ğ“ª', 'ğ”', 'ğ•’', 'ğ–†', 'ğ–º', 'ğ—®', 'ğ˜¢', 'ğ™–', 'ğšŠ', 'â‚', 'áµƒ', 'áµ„', 'ğŸ„°', 'ğŸ…', 'ğŸ…°', 'ğŸ…š', 'ğŸ…«', 'ğŸ…°ï¸', 'ğŸ„°ï¸'],
    'b': ['b', '8', '6', 'Î²', '*', 'â“‘', 'â’·', 'ï½‚', 'ï¼¢', 'á¸ƒ', 'á¸…', 'á¸‡', 'Ê™', 'É“', 'Ğ¬', 'ÃŸ', 'ğ›', 'ğ’ƒ', 'ğ’·', 'ğ“«', 'ğ”Ÿ', 'ğ•“', 'ğ–‡', 'ğ–»', 'ğ—¯', 'ğ˜£', 'ğ™—', 'ğš‹', 'áµ‡', 'ğŸ„±', 'ğŸ…‘', 'ğŸ…±', 'ğŸ…›', 'ğŸ…¬', 'ğŸ…±ï¸', 'ğŸ„±ï¸'],
    'c': ['c', '(', '<', 'Ã§', '*', 'â“’', 'â’¸', 'ï½ƒ', 'ï¼£', 'Ä‡', 'Ä‰', 'Ä‹', 'Ä', 'á´„', 'É”', 'Â¢', 'ğœ', 'ğ’„', 'ğ’¸', 'ğ“¬', 'ğ” ', 'ğ•”', 'ğ–ˆ', 'ğ–¼', 'ğ—°', 'ğ˜¤', 'ğ™˜', 'ğšŒ', 'á¶œ', 'ğŸ„²', 'ğŸ…’', 'ğŸ…²', 'ğŸ…œ', 'ğŸ…­', 'ğŸ…²ï¸', 'ğŸ„²ï¸'],
    'd': ['d', '|)', 'â““', 'â’¹', 'ï½„', 'ï¼¤', 'Ä', 'Ä‘', 'á´…', 'É–', 'Ã°', 'ğ', 'ğ’…', 'ğ’¹', 'ğ“­', 'ğ”¡', 'ğ••', 'ğ–‰', 'ğ–½', 'ğ—±', 'ğ˜¥', 'ğ™™', 'ğš', 'áµˆ', 'ğŸ„³', 'ğŸ…“', 'ğŸ…³', 'ğŸ…', 'ğŸ…®', 'ğŸ…³ï¸', 'ğŸ„³ï¸'],
    'e': ['e', '3', 'â‚¬', 'Îµ', '*', 'â“”', 'â’º', 'ï½…', 'ï¼¥', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ä“', 'Ä•', 'Ä—', 'Ä™', 'Ä›', 'á´‡', 'É˜', 'Â£', 'ğ', 'ğ’†', 'â„¯', 'ğ“®', 'ğ”¢', 'ğ•–', 'ğ–Š', 'ğ–¾', 'ğ—²', 'ğ˜¦', 'ğ™š', 'ğš', 'â‚‘', 'áµ‰', 'ğŸ„´', 'ğŸ…”', 'ğŸ…´', 'ğŸ…', 'ğŸ…¯', 'ğŸ…´ï¸', 'ğŸ„´ï¸'],
    'f': ['f', 'Æ’', 'â“•', 'â’»', 'ï½†', 'ï¼¦', 'êœ°', 'Ê„', 'Êƒ', 'ğŸ', 'ğ’‡', 'ğ’»', 'ğ“¯', 'ğ”£', 'ğ•—', 'ğ–‹', 'ğ–¿', 'ğ—³', 'ğ˜§', 'ğ™›', 'ğš', 'á¶ ', 'ğŸ„µ', 'ğŸ…•', 'ğŸ…µ', 'ğŸ…Ÿ', 'ğŸ…°', 'ğŸ…µï¸', 'ğŸ„µï¸'],
    'g': ['g', '9', 'â“–', 'â’¼', 'ï½‡', 'ï¼§', 'Ä', 'ÄŸ', 'Ä¡', 'Ä£', 'É¢', 'É¡', 'ğ ', 'ğ’ˆ', 'â„Š', 'ğ“°', 'ğ”¤', 'ğ•˜', 'ğ–Œ', 'ğ—€', 'ğ—´', 'ğ˜¨', 'ğ™œ', 'ğš', 'áµ', 'ğŸ„¶', 'ğŸ…–', 'ğŸ…¶', 'ğŸ… ', 'ğŸ…±', 'ğŸ…¶ï¸', 'ğŸ„¶ï¸'],
    'h': ['h', '#', 'â“—', 'â’½', 'ï½ˆ', 'ï¼¨', 'Ä¥', 'Ä§', 'Êœ', 'É¦', 'Ğ½', 'ğ¡', 'ğ’‰', 'ğ’½', 'ğ“±', 'ğ”¥', 'ğ•™', 'ğ–', 'ğ—', 'ğ—µ', 'ğ˜©', 'ğ™', 'ğš‘', 'â‚•', 'Ê°', 'ğŸ„·', 'ğŸ…—', 'ğŸ…·', 'ğŸ…¡', 'ğŸ…²', 'ğŸ…·ï¸', 'ğŸ„·ï¸'],
    'i': ['i', '1', '!', '|', 'Î¹', '*', 'â“˜', 'â’¾', 'ï½‰', 'ï¼©', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ä©', 'Ä«', 'Ä­', 'Ä¯', 'Ä±', 'Éª', 'É¨', 'Â¡', 'ğ¢', 'ğ’Š', 'ğ’¾', 'ğ“²', 'ğ”¦', 'ğ•š', 'ğ–', 'ğ—‚', 'ğ—¶', 'ğ˜ª', 'ğ™', 'ğš’', 'áµ¢', 'â±', 'ğŸ„¸', 'ğŸ…˜', 'ğŸ…¸', 'ğŸ…¢', 'ğŸ…³', 'ğŸ…¸ï¸', 'ğŸ„¸ï¸'],
    'j': ['j', 'â“™', 'â’¿', 'ï½Š', 'ï¼ª', 'Äµ', 'á´Š', 'Ê', 'Ù„', 'ğ£', 'ğ’‹', 'ğ’¿', 'ğ“³', 'ğ”§', 'ğ•›', 'ğ–', 'ğ—ƒ', 'ğ—·', 'ğ˜«', 'ğ™Ÿ', 'ğš“', 'Ê²', 'ğŸ„¹', 'ğŸ…™', 'ğŸ…¹', 'ğŸ…£', 'ğŸ…´', 'ğŸ…¹ï¸', 'ğŸ„¹ï¸'],
    'k': ['k', 'â“š', 'â“€', 'ï½‹', 'ï¼«', 'Ä·', 'á´‹', 'Ê', 'Îº', 'ğ¤', 'ğ’Œ', 'ğ“€', 'ğ“´', 'ğ”¨', 'ğ•œ', 'ğ–', 'ğ—„', 'ğ—¸', 'ğ˜¬', 'ğ™ ', 'ğš”', 'â‚–', 'áµ', 'ğŸ„º', 'ğŸ…š', 'ğŸ…º', 'ğŸ…¤', 'ğŸ…µ', 'ğŸ…ºï¸', 'ğŸ„ºï¸'],
    'l': ['l', '|', '*', 'â“›', 'â“', 'ï½Œ', 'ï¼¬', 'Äº', 'Ä¼', 'Ä¾', 'Å€', 'Å‚', 'ÊŸ', 'É­', 'Â£', 'ğ¥', 'ğ’', 'ğ“', 'ğ“µ', 'ğ”©', 'ğ•', 'ğ–‘', 'ğ—…', 'ğ—¹', 'ğ˜­', 'ğ™¡', 'ğš•', 'â‚—', 'Ë¡', 'ğŸ„»', 'ğŸ…›', 'ğŸ…»', 'ğŸ…¥', 'ğŸ…¶', 'ğŸ…»ï¸', 'ğŸ„»ï¸', '1', 'I', 'i'],
    'm': ['m', 'â“œ', 'â“‚', 'ï½', 'ï¼­', 'á´', 'É±', 'Ğ¼', 'ğ¦', 'ğ’', 'ğ“‚', 'ğ“¶', 'ğ”ª', 'ğ•', 'ğ–’', 'ğ—†', 'ğ—º', 'ğ˜®', 'ğ™¢', 'ğš–', 'áµ', 'ğŸ„¼', 'ğŸ…œ', 'ğŸ…¼', 'ğŸ…¦', 'ğŸ…·', 'ğŸ…¼ï¸', 'ğŸ„¼ï¸'],
    'n': ['n', 'â“', 'â“ƒ', 'ï½', 'ï¼®', 'Ã±', 'Å„', 'Å†', 'Åˆ', 'Å‰', 'É´', 'É²', 'Ğ¸', 'ğ§', 'ğ’', 'ğ“ƒ', 'ğ“·', 'ğ”«', 'ğ•Ÿ', 'ğ–“', 'ğ—‡', 'ğ—»', 'ğ˜¯', 'ğ™£', 'ğš—', 'â‚™', 'â¿', 'ğŸ„½', 'ğŸ…', 'ğŸ…½', 'ğŸ…§', 'ğŸ…¸', 'ğŸ…½ï¸', 'ğŸ„½ï¸'],
    'o': ['o', '0', '()', 'Î¿', '*', 'â“', 'â“„', 'ï½', 'ï¼¯', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Å', 'Å', 'Å‘', 'á´', 'Éµ', 'Î¸', 'ğ¨', 'ğ’', 'â„´', 'ğ“¸', 'ğ”¬', 'ğ• ', 'ğ–”', 'ğ—ˆ', 'ğ—¼', 'ğ˜°', 'ğ™¤', 'ğš˜', 'â‚’', 'áµ’', 'ğŸ„¾', 'ğŸ…', 'ğŸ…¾', 'ğŸ…¨', 'ğŸ…¹', 'ğŸ…¾ï¸', 'ğŸ„¾ï¸'],
    'p': ['p', 'â“Ÿ', 'â“…', 'ï½', 'ï¼°', 'á´˜', 'Æ¥', 'Ï', 'ğ©', 'ğ’‘', 'ğ“¹', 'ğ”­', 'ğ•¡', 'ğ–•', 'ğ—‰', 'ğ—½', 'ğ˜±', 'ğ™¥', 'ğš™', 'áµ–', 'ğŸ„¿', 'ğŸ…Ÿ', 'ğŸ…¿', 'ğŸ…©', 'ğŸ…º', 'ğŸ…¿ï¸', 'ğŸ„¿ï¸'],
    'q': ['q', 'â“ ', 'â“†', 'ï½‘', 'ï¼±', 'Ï™', 'Ê ', 'ğª', 'ğ’’', 'ğ“†', 'ğ“º', 'ğ”®', 'ğ•¢', 'ğ––', 'ğ—Š', 'ğ—¾', 'ğ˜²', 'ğ™¦', 'ğšš', 'ğŸ…€', 'ğŸ… ', 'ğŸ†€', 'ğŸ…»', 'ğŸ†€ï¸', 'ğŸ…€ï¸'],
    'r': ['r', 'â“¡', 'â“‡', 'ï½’', 'ï¼²', 'Å•', 'Å—', 'Å™', 'Ê€', 'É¹', 'Ñ', 'ğ«', 'ğ’“', 'ğ“‡', 'ğ“»', 'ğ”¯', 'ğ•£', 'ğ–—', 'ğ—‹', 'ğ—¿', 'ğ˜³', 'ğ™§', 'ğš›', 'áµ£', 'Ê³', 'ğŸ…', 'ğŸ…¡', 'ğŸ†', 'ğŸ…¼', 'ğŸ†ï¸', 'ğŸ…ï¸'],
    's': ['s', '5', '$', '*', 'â“¢', 'â“ˆ', 'ï½“', 'ï¼³', 'Å›', 'Å', 'ÅŸ', 'Å¡', 'Å¿', 'êœ±', 'Ê‚', 'Â§', 'ğ¬', 'ğ’”', 'ğ“ˆ', 'ğ“¼', 'ğ”°', 'ğ•¤', 'ğ–˜', 'ğ—Œ', 'ğ˜€', 'ğ˜´', 'ğ™¨', 'ğšœ', 'â‚›', 'Ë¢', 'ğŸ…‚', 'ğŸ…¢', 'ğŸ†‚', 'ğŸ…½', 'ğŸ†‚ï¸', 'ğŸ…‚ï¸'],
    't': ['t', '7', '+', 'Ï„', '*', 'â“£', 'â“‰', 'ï½”', 'ï¼´', 'Å£', 'Å¥', 'Å§', 'á´›', 'Êˆ', 'â€ ', 'ğ­', 'ğ’•', 'ğ“‰', 'ğ“½', 'ğ”±', 'ğ•¥', 'ğ–™', 'ğ—', 'ğ˜', 'ğ˜µ', 'ğ™©', 'ğš', 'â‚œ', 'áµ—', 'ğŸ…ƒ', 'ğŸ…£', 'ğŸ†ƒ', 'ğŸ…¾', 'ğŸ†ƒï¸', 'ğŸ…ƒï¸'],
    'u': ['u','@', 'v', 'Ï…', '*', 'â“¤', 'â“Š', 'ï½•', 'ï¼µ', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Å©', 'Å«', 'Å­', 'Å¯', 'Å±', 'Å³', 'á´œ', 'ÊŠ', 'Âµ', 'ğ®', 'ğ’–', 'ğ“Š', 'ğ“¾', 'ğ”²', 'ğ•¦', 'ğ–š', 'ğ—', 'ğ˜‚', 'ğ˜¶', 'ğ™ª', 'ğš', 'áµ¤', 'áµ˜', 'ğŸ…„', 'ğŸ…¤', 'ğŸ†„', 'ğŸ…¿', 'ğŸ†„ï¸', 'ğŸ…„ï¸'],
    'v': ['v', 'u', '*', 'â“¥', 'â“‹', 'ï½–', 'ï¼¶', 'á´ ', 'Ê‹', 'Ñµ', 'ğ¯', 'ğ’—',
'ğ“‹', 'ğ“¿', 'ğ”³', 'ğ•§', 'ğ–›', 'ğ—', 'ğ˜ƒ', 'ğ˜·', 'ğ™«', 'ğšŸ', 'áµ›', 'ğŸ……', 'ğŸ…¥', 'ğŸ†…', 'ğŸ†…ï¸', 'ğŸ……ï¸'],
    'w': ['w', 'â“¦', 'â“Œ', 'ï½—', 'ï¼·', 'á´¡', 'Ê', 'Ï‰', 'ğ°', 'ğ’˜', 'ğ“Œ', 'ğ”€', 'ğ”´', 'ğ•¨', 'ğ–œ', 'ğ—', 'ğ˜„', 'ğ˜¸', 'ğ™¬', 'ğš ', 'Ê·', 'ğŸ…†', 'ğŸ…¦', 'ğŸ††', 'ğŸ††ï¸', 'ğŸ…†ï¸', 'vv', 'uu'],
    'x': ['x', 'Ã—', '*', 'â“§', 'â“', 'ï½˜', 'ï¼¸', 'á™®', 'Ï‡', 'Ğ¶', 'ğ±', 'ğ’™', 'ğ“', 'ğ”', 'ğ”µ', 'ğ•©', 'ğ–', 'ğ—‘', 'ğ˜…', 'ğ˜¹', 'ğ™­', 'ğš¡', 'Ë£', 'ğŸ…‡', 'ğŸ…§', 'ğŸ†‡', 'ğŸ†‡ï¸', 'ğŸ…‡ï¸', '><'],
    'y': ['y', 'â“¨', 'â“', 'ï½™', 'ï¼¹', 'Ã½', 'Ã¿', 'Å·', 'Ê', 'É£', 'Ñƒ', 'ğ²', 'ğ’š', 'ğ“', 'ğ”‚', 'ğ”¶', 'ğ•ª', 'ğ–', 'ğ—’', 'ğ˜†', 'ğ˜º', 'ğ™®', 'ğš¢', 'Ê¸', 'ğŸ…ˆ', 'ğŸ…¨', 'ğŸ†ˆ', 'ğŸ†ˆï¸', 'ğŸ…ˆï¸'],
    'z': ['z', '2', '*', 'â“©', 'â“', 'ï½š', 'ï¼º', 'Åº', 'Å¼', 'Å¾', 'á´¢', 'Ê', 'Ğ·', 'ğ³', 'ğ’›', 'ğ“', 'ğ”ƒ', 'ğ”·', 'ğ•«', 'ğ–Ÿ', 'ğ—“', 'ğ˜‡', 'ğ˜»', 'ğ™¯', 'ğš£', 'á¶»', 'ğŸ…‰', 'ğŸ…©', 'ğŸ†‰', 'ğŸ†‰ï¸', 'ğŸ…‰ï¸'],
    
    # Uppercase letters 
    'A': ['A', 'ğ€', 'ğ‘¨', 'ğ’œ', 'ğ“', 'ğ”„', 'ğ”¸', 'ğ•¬', 'ğ– ', 'ğ—”', 'ğ˜ˆ', 'ğ˜¼', 'ğ™°', 'ğŸ„', 'ğŸ…', 'ğŸ…°', '4', '@'],
    'B': ['B', 'ğ', 'ğ‘©', 'â„¬', 'ğ“‘', 'ğ”…', 'ğ”¹', 'ğ•­', 'ğ–¡', 'ğ—•', 'ğ˜‰', 'ğ˜½', 'ğ™±', 'ğŸ„‘', 'ğŸ…‘', 'ğŸ…±', '8', '6'],
    'C': ['C', 'ğ‚', 'ğ‘ª', 'ğ’', 'ğ“’', 'â„­', 'â„‚', 'ğ•®', 'ğ–¢', 'ğ—–', 'ğ˜Š', 'ğ˜¾', 'ğ™²', 'ğŸ„’', 'ğŸ…’', 'ğŸ…²', '(', '<'],
    'D': ['D', 'ğƒ', 'ğ‘«', 'ğ’Ÿ', 'ğ““', 'ğ”‡', 'ğ”»', 'ğ•¯', 'ğ–£', 'ğ——', 'ğ˜‹', 'ğ˜¿', 'ğ™³', 'ğŸ„“', 'ğŸ…“', 'ğŸ…³'],
    'E': ['E', 'ğ„', 'ğ‘¬', 'â„°', 'ğ“”', 'ğ”ˆ', 'ğ”¼', 'ğ•°', 'ğ–¤', 'ğ—˜', 'ğ˜Œ', 'ğ™€', 'ğ™´', 'ğŸ„”', 'ğŸ…” ','ğŸ…´', '3', 'â‚¬'],
    'F': ['F', 'ğ…', 'ğ‘­', 'â„±', 'ğ“•', 'ğ”‰', 'ğ”½', 'ğ•±', 'ğ–¥', 'ğ—™', 'ğ˜', 'ğ™', 'ğ™µ', 'ğŸ„•', 'ğŸ…•', 'ğŸ…µ'],
    'G': ['G', 'ğ†', 'ğ‘®', 'ğ’¢', 'ğ“–', 'ğ”Š', 'ğ”¾', 'ğ•²', 'ğ–¦', 'ğ—š', 'ğ˜', 'ğ™‚', 'ğ™¶', 'ğŸ„–', 'ğŸ…–', 'ğŸ…¶', '9'],
    'H': ['H', 'ğ‡', 'ğ‘¯', 'â„‹', 'ğ“—', 'â„Œ', 'â„', 'ğ•³', 'ğ–§', 'ğ—›', 'ğ˜', 'ğ™ƒ', 'ğ™·', 'ğŸ„—', 'ğŸ…—', 'ğŸ…·', '#'],
    'I': ['I', 'ğˆ', 'ğ‘°', 'â„', 'ğ“˜', 'â„‘', 'ğ•€', 'ğ•´', 'ğ–¨', 'ğ—œ', 'ğ˜', 'ğ™„', 'ğ™¸', 'ğŸ„˜', 'ğŸ…˜', 'ğŸ…¸', '1', '!', '|', 'l', 'i'],
    'J': ['J', 'ğ‰', 'ğ‘±', 'ğ’¥', 'ğ“™', 'ğ”', 'ğ•', 'ğ•µ', 'ğ–©', 'ğ—', 'ğ˜‘', 'ğ™…', 'ğ™¹', 'ğŸ„™', 'ğŸ…™', 'ğŸ…¹'],
    'K': ['K', 'ğŠ', 'ğ‘²', 'ğ’¦', 'ğ“š', 'ğ”', 'ğ•‚', 'ğ•¶', 'ğ–ª', 'ğ—', 'ğ˜’', 'ğ™†', 'ğ™º', 'ğŸ„š', 'ğŸ…š', 'ğŸ…º'],
    'L': ['L', 'ğ‹', 'ğ‘³', 'â„’', 'ğ“›', 'ğ”', 'ğ•ƒ', 'ğ•·', 'ğ–«', 'ğ—Ÿ', 'ğ˜“', 'ğ™‡', 'ğ™»', 'ğŸ„›', 'ğŸ…›', 'ğŸ…»', '1', 'I', 'i', '|'],
    'M': ['M', 'ğŒ', 'ğ‘´', 'â„³', 'ğ“œ', 'ğ”', 'ğ•„', 'ğ•¸', 'ğ–¬', 'ğ— ', 'ğ˜”', 'ğ™ˆ', 'ğ™¼', 'ğŸ„œ', 'ğŸ…œ', 'ğŸ…¼'],
    'N': ['N', 'ğ', 'ğ‘µ', 'ğ’©', 'ğ“', 'ğ”‘', 'â„•', 'ğ•¹', 'ğ–­', 'ğ—¡', 'ğ˜•', 'ğ™‰', 'ğ™½', 'ğŸ„', 'ğŸ…', 'ğŸ…½'],
    'O': ['O', 'ğ', 'ğ‘¶', 'ğ’ª', 'ğ“', 'ğ”’', 'ğ•†', 'ğ•º', 'ğ–®', 'ğ—¢', 'ğ˜–', 'ğ™Š', 'ğ™¾', 'ğŸ„', 'ğŸ…', 'ğŸ…¾', '0', '()'],
    'P': ['P', 'ğ', 'ğ‘·', 'ğ’«', 'ğ“Ÿ', 'ğ”“', 'â„™', 'ğ•»', 'ğ–¯', 'ğ—£', 'ğ˜—', 'ğ™‹', 'ğ™¿', 'ğŸ„Ÿ', 'ğŸ…Ÿ', 'ğŸ†€'],
    'Q': ['Q', 'ğ', 'ğ‘¸', 'ğ’¬', 'ğ“ ', 'ğ””', 'â„š', 'ğ•¼', 'ğ–°', 'ğ—¤', 'ğ˜˜', 'ğ™Œ', 'ğš€', 'ğŸ„ ', 'ğŸ… ', 'ğŸ†'],
    'R': ['R', 'ğ‘', 'ğ‘¹', 'â„›', 'ğ“¡', 'â„œ', 'â„', 'ğ•½', 'ğ–±', 'ğ—¥', 'ğ˜™', 'ğ™', 'ğš', 'ğŸ„¡', 'ğŸ…¡', 'ğŸ†‚'],
    'S': ['S', 'ğ’', 'ğ‘º', 'ğ’®', 'ğ“¢', 'ğ”–', 'ğ•Š', 'ğ•¾', 'ğ–²', 'ğ—¦', 'ğ˜š', 'ğ™', 'ğš‚', 'ğŸ„¢', 'ğŸ…¢', 'ğŸ†ƒ', '5', '$'],
    'T': ['T', 'ğ“', 'ğ‘»', 'ğ’¯', 'ğ“£', 'ğ”—', 'ğ•‹', 'ğ•¿', 'ğ–³', 'ğ—§', 'ğ˜›', 'ğ™', 'ğšƒ', 'ğŸ„£', 'ğŸ…£', 'ğŸ†„', '7', '+'],
    'U': ['U', 'ğ”', 'ğ‘¼', 'ğ’°', 'ğ“¤', 'ğ”˜', 'ğ•Œ', 'ğ–€', 'ğ–´', 'ğ—¨', 'ğ˜œ', 'ğ™', 'ğš„', 'ğŸ„¤', 'ğŸ…¤', 'ğŸ†„', 'V'],
    'V': ['V', 'ğ•', 'ğ‘½', 'ğ’±', 'ğ“¥', 'ğ”™', 'ğ•', 'ğ–', 'ğ–µ', 'ğ—©', 'ğ˜', 'ğ™‘', 'ğš…', 'ğŸ„¥', 'ğŸ…¥', 'ğŸ†…', 'U'],
    'W': ['W', 'ğ–', 'ğ‘¾', 'ğ’²', 'ğ“¦', 'ğ”š', 'ğ•', 'ğ–‚', 'ğ–¶', 'ğ—ª', 'ğ˜', 'ğ™’', 'ğš†', 'ğŸ„¦', 'ğŸ…¦', 'ğŸ†‡', 'VV', 'UU'],
    'X': ['X', 'ğ—', 'ğ‘¿', 'ğ’³', 'ğ“§', 'ğ”›', 'ğ•', 'ğ–ƒ', 'ğ–·', 'ğ—«', 'ğ˜Ÿ', 'ğ™“', 'ğš‡', 'ğŸ„§', 'ğŸ…§', 'ğŸ†ˆ', '><'],
    'Y': ['Y', 'ğ˜', 'ğ’€', 'ğ’´', 'ğ“¨', 'ğ”œ', 'ğ•', 'ğ–„', 'ğ–¸', 'ğ—¬', 'ğ˜ ', 'ğ™”', 'ğšˆ', 'ğŸ„¨', 'ğŸ…¨', 'ğŸ†‰'],
    'Z': ['Z', 'ğ™', 'ğ’', 'ğ’µ', 'ğ“©', 'â„¨', 'â„¤', 'ğ–…', 'ğ–¹', 'ğ—­', 'ğ˜¡', 'ğ™•', 'ğš‰', 'ğŸ„©', 'ğŸ…©', 'ğŸ†Š'],

    '0': ['0', 'â“ª', 'ï¼', 'ğŸ', 'ğŸ˜', 'ğŸ¢', 'ğŸ¬', 'ğŸ¶', 'o', 'O', '()', 'Î¿', '*', 'â“', 'â“„', 'ï½', 'ï¼¯', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Å', 'Å', 'Å‘', 'á´', 'Éµ', 'Î¸'],
    '1': ['1', 'â‘ ', 'ï¼‘', 'ğŸ', 'ğŸ™', 'ğŸ£', 'ğŸ­', 'ğŸ·', 'I', 'i', '!', '|', 'Î¹', '*', 'â“˜', 'â’¾', 'ï½‰', 'ï¼©', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ä©', 'Ä«', 'Ä­', 'Ä¯', 'Ä±', 'Éª', 'É¨', 'Â¡'],
    '2': ['2', 'â‘¡', 'ï¼’', 'ğŸ', 'ğŸš', 'ğŸ¤', 'ğŸ®', 'ğŸ¸', 'z', 'Z', '*', 'â“©', 'â“', 'ï½š', 'ï¼º', 'Åº', 'Å¼', 'Å¾', 'á´¢', 'Ê', 'Ğ·'],
    '3': ['3', 'â‘¢', 'ï¼“', 'ğŸ‘', 'ğŸ›', 'ğŸ¥', 'ğŸ¯', 'ğŸ¹', 'e', 'E', 'â‚¬', 'Îµ', '*', 'â“”', 'â’º', 'ï½…', 'ï¼¥', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ä“', 'Ä•', 'Ä—', 'Ä™', 'Ä›', 'á´‡', 'É˜', 'Â£'],
    '4': ['4', 'â‘£', 'ï¼”', 'ğŸ’', 'ğŸœ', 'ğŸ¦', 'ğŸ°', 'ğŸº', 'a', 'A', '@', 'Î±', 'Î»', '*', 'â“', 'â’¶', 'ï½', 'ï¼¡', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'á´€', 'É', 'É’', 'Ğ”', 'Ã…', 'h'],
    '5': ['5', 'â‘¤', 'ï¼•', 'ğŸ“', 'ğŸ', 'ğŸ§', 'ğŸ±', 'ğŸ»', 's', 'S', '*', 'â“¢', 'â“ˆ', 'ï½“', 'ï¼³', 'Å›', 'Å', 'ÅŸ', 'Å¡', 'Å¿', 'êœ±', 'Ê‚', 'Â§'],
    '6': ['6', 'â‘¥', 'ï¼–', 'ğŸ”', 'ğŸ', 'ğŸ¨', 'ğŸ²', 'ğŸ¼', 'b', 'B', '8', 'Î²', '*', 'â“‘', 'â’·', 'ï½‚', 'ï¼¢', 'á¸ƒ', 'á¸…', 'á¸‡', 'Ê™', 'É“', 'Ğ¬', 'ÃŸ'],
    '7': ['7', 'â‘¦', 'ï¼—', 'ğŸ•', 'ğŸŸ', 'ğŸ©', 'ğŸ³', 'ğŸ½', 't', 'T', '+', 'Ï„', '*', 'â“£', 'â“‰', 'ï½”', 'ï¼´', 'Å£', 'Å¥', 'Å§', 'á´›', 'Êˆ', 'â€ '],
    '8': ['8', 'â‘§', 'ï¼˜', 'ğŸ–', 'ğŸ ', 'ğŸª', 'ğŸ´', 'ğŸ¾', 'b', 'B', '6', 'Î²', '*', 'â“‘', 'â’·', 'ï½‚', 'ï¼¢', 'á¸ƒ', 'á¸…', 'á¸‡', 'Ê™', 'É“', 'Ğ¬', 'ÃŸ'],
    '9': ['9', 'â‘¨', 'ï¼™', 'ğŸ—', 'ğŸ¡', 'ğŸ«', 'ğŸµ', 'ğŸ¿', 'g', 'G', 'â“–', 'â’¼', 'ï½‡', 'ï¼§', 'Ä', 'ÄŸ', 'Ä¡', 'Ä£', 'É¢', 'É¡']
}

for base, variants in COMBINED_SUBSTITUTIONS.items():
    COMBINED_SUBSTITUTIONS[base] = sorted(set(variants), key=lambda x: (len(x), x))

CLEAN_BRACKETS = {"[", "]", "{", "}", "(", ")"}

# === HOMOGLYPHS
HOMOGLYPHS = {
    'Ñ•': 's', 'Ñ': 'c', 'Ğµ': 'e', 'Ğ°': 'a', 'Ñ€': 'p', 'Ğ¾': 'o', 'Ñ–': 'i',
    'Ô': 'd', 'Ó': 'l', 'Ò»': 'h', 'Ô›': 'q', 'á´€': 'a', 'Ê™': 'b', 'á´„': 'c',
    'á´…': 'd', 'á´‡': 'e', 'Ò“': 'f', 'É¢': 'g', 'Ğ½': 'h', 'Éª': 'i', 'á´Š': 'j',
    'á´‹': 'k', 'ÊŸ': 'l', 'á´': 'm', 'É´': 'n', 'á´': 'o', 'á´˜': 'p', 'Ç«': 'q',
    'Ê€': 'r', 'Ñ•': 's', 'á´›': 't', 'á´œ': 'u', 'á´ ': 'v', 'á´¡': 'w', 'Ê': 'y', 'á´¢': 'z'
}

# === Build Reverse Map
REVERSE_SUBSTITUTIONS = defaultdict(set)
for base, variants in COMBINED_SUBSTITUTIONS.items():
    for v in variants:
        for form in {v, v.lower(), v.upper()}:
            REVERSE_SUBSTITUTIONS[form].add(base.lower())

# === Text Preprocessing
def squash_repeats(text: str, threshold: int = 2) -> str:
    return re.sub(r'(.)\1{' + str(threshold - 1) + r',}', r'\1', text)

def normalize_homoglyphs(text: str) -> str:
    return ''.join(HOMOGLYPHS.get(c, c) for c in text)

def strip_nonalpha_punct(text: str) -> str:
    return re.sub(r'[^a-zA-Z0-9\s]', '', text)

def collapse_spaced_letters(text: str) -> str:
    return re.sub(r'(?i)\b(?:[a-z]\s+){2,}[a-z]\b', lambda m: m.group(0).replace(' ', ''), text)

def preprocess_text_for_filtering(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = normalize_homoglyphs(text)
    text = squash_repeats(text)
    text = collapse_spaced_letters(text)
    text = strip_nonalpha_punct(text)
    return text.lower().strip()
# ==================== Normalization + Preprocessing ====================

def build_normalization_map(substitutions: Dict[str, List[str]]) -> Dict[str, str]:
    """Create a consistent normalization map from all variants to their base characters."""
    norm_map = {}
    for base, variants in substitutions.items():
        for var in variants:
            for form in {var, var.lower(), var.upper()}:
                if form not in norm_map:
                    norm_map[form] = base.lower()
    return norm_map

NORMALIZATION_MAP = build_normalization_map(COMBINED_SUBSTITUTIONS)

def expand_all_normalizations(word: str, max_variants: int = 50000) -> set:
    possibilities = []
    for char in word:
        options = list(REVERSE_SUBSTITUTIONS.get(char, {char}))
        possibilities.append(sorted(set(options), key=lambda x: (len(x), x)))

    all_combos = set()
    for combo in product(*possibilities):
        all_combos.add(''.join(combo))
        if len(all_combos) >= max_variants:
            break
    return all_combos

def remove_hidden_chars(text: str) -> str:
    """Remove invisible characters that can be used to bypass filters."""
    for char in HIDDEN_SEPARATORS:
        text = text.replace(char, '')
    return text

def normalize_to_base(text: str) -> str:
    """Replace obfuscated variants using regex â€” supports symbols & multichars."""
    sorted_variants = sorted(NORMALIZATION_MAP.items(), key=lambda x: -len(x[0]))
    for variant, base in sorted_variants:
        try:
            text = re.sub(re.escape(variant), base, text, flags=re.IGNORECASE)
        except Exception as e:
            print(f"Sub error: {variant} -> {base} = {e}")
    return text

def squeeze_text(text: str) -> str:
    """Remove all non-alphanumeric characters (like spaces, dots, dashes, etc)."""
    return re.sub(r'[^a-zA-Z0-9]', '', text)


CONTEXT_WHITELIST = {
    'cunt': {
        'patterns': [
            r'\bcunt(?:ry|ries|ing|ed|ious|ure|ship)\b',
            r'\b(?:dis|re)cunt\b',
            r'\bcount\b',
            r'\baccount\b'
        ],
        'timeout': 1.0},
    'ass': {
        'patterns': [
            r'\bass\s*(?:ignment|essment|ociation|embly|ets|ist|uming|ert)',
            r'\b(?:cl|gr|m|p)ass\b',
            r'\b(?:embr|harr)ass'
        ],
        'timeout': 1.0
    },
    'cock': {
        'patterns': [
            r'\bcock(?:tail|atoo|pit|roach)',
            r'\bpea(?:cock)\b',
            r'\bhancock\b',
            r'\bshuttle(?:cock)\b'
        ],
        'timeout': 1.0
    },
    'hell': {
        'patterns': [
            r'\bhell(?:o|icopter|met|ium|enic)',
            r'\bshell\b',
            r'\bothello\b'
        ],
        'timeout': 1.0
    }
}

SUFFIX_RULES = {
    'ing': {'min_length': 4, 'exceptions': set(['ring', 'king', 'sing'])},
    'er': {'min_length': 3, 'exceptions': set(['her', 'per'])},
    'ed': {'min_length': 3, 'exceptions': set(['red', 'bed'])},
    'a': {'min_length': 4, 'exceptions': set(['banana'])} , # New rule for cases like 'fucka'
    's': {'min_length': 3, 'exceptions': set(['is', 'as', 'us'])},
    'es': {'min_length': 4, 'exceptions': set(['yes', 'res', 'des'])}
}

COMMON_SAFE_WORDS = {
    "penistone", "lightwater", "cockburn", "mianus",
    "hello","tatsuki", "cumming", "clitheroe", "twatt", "fanny", "assington", "bitchfield",
    "titcomb", "rape", "shitterton", "prickwillow", "whale", "beaver",
    "cocktail", "passage", "classic", "grassland", "bassist", "butterfly",
    "shipment", "shooting", "language", "counting", "cluster", "glassware",
    "testes", "scrotum", "vaginal", "urethra", "mastectomy", "vasectomy",
    "nucleus", "molecular", "pascal", "vascular", "fascial",
    "clitheroe", "twatt", "fanny", "assington", "bitchfield", "titcomb",
    "shitterton", "prickwillow", "cockermouth", "cockbridge"
}

SHORT_SWEARS = {
    'fx', 'fk', 'sht', 'wtf', 'ffs', 'ngr', 'bch', 'cnt', 'dck',
    'fck', 'sh1', '5ht', 'vgn', 'prn', 'f4n', 'n1g', 'k3k', 'fku',
    'sht', 'ass', 'bch', 'cnt', 'dck', 'fuk', 'fuc', 'sh1', 'fgs',
    'ngr', 'wth', 'dmn', 'bch', 'cnt', 'dmn', 'fgs', 'ngr', 'prk',
    'twt', 'vgn', 'prn', 'f4n', 'n1g', 'k3k', 'fku', 'sht'
}

# ==================== UTILITY FUNCTIONS ====================


def simple_metaphone(s: str, max_length: int = 8) -> str:
    """Improved phonetic algorithm to catch misspellings like 'fukc'"""
    if not s:
        return ""
    
    s = s.lower()
    # First normalize to base characters
    s = normalize_to_base(s)
    
    replacements = [
        (r'[^a-z]', ''),          # Remove non-letters
        (r'([aeiou])h', r'\1'),    # vowel+h â†’ vowel
        (r'gh(?=[iey])', ''),      # silent gh
        (r'ck', 'k'),              # ck â†’ k
        (r'c(?!e|i|y)', 'k'),      # Hard c â†’ k
        (r'ph', 'f'),              # ph â†’ f
        (r'qu', 'kw'),             # qu â†’ kw
        (r'x', 'ks'),              # x â†’ ks
        (r'(\w)\1+', r'\1'),       # Remove duplicates
        (r'sch', 'sk'),            # sch â†’ sk
        (r'th', 't'),              # th â†’ t
        (r'^kn', 'n'),             # silent k
        (r'^gn', 'n'),             # silent g
        (r'^pn', 'n'),             # silent p
        (r'^wr', 'r'),             # silent w
        (r'mb$', 'm'),             # silent b
        # Additional rules for common misspellings
        (r'([^s]|^)c(?=[iey])', r'\1s'),  # câ†’s before e,i,y (except after s)
        (r'([^f]|^)gh', r'\1g'),   # ghâ†’g (except after f)
        (r'([^t]|^)ch', r'\1k'),   # châ†’k (except after t)
    ]
    
    for pattern, repl in replacements:
        s = re.sub(pattern, repl, s)
    
    # Sort consonants to catch transpositions
    if len(s) >= 4:
        first = s[0]
        last = s[-1]
        middle = ''.join(sorted(s[1:-1])) if len(s) > 2 else ''
        s = first + middle + last
    
    return s[:max_length]

def split_words(input_text: str) -> List[str]:
    """Split input into words (handles both comma and space separated words)
    
    Examples:
        "word1 word2 word3" â†’ ["word1", "word2", "word3"]
        "word1,word2,word3" â†’ ["word1", "word2", "word3"]
        "word1, word2, word3" â†’ ["word1", "word2", "word3"]
        "word1 word2,word3" â†’ ["word1", "word2", "word3"]
    """
    text = input_text.lower()
    # Remove all special characters except letters, numbers, commas and spaces
    text = re.sub(r"[^a-z0-9,\s]", "", text)
    
    # Split by both commas and whitespace, then clean each word
    words = []
    for word in re.split(r"[, \s]+", text):
        word = word.strip()
        if word:
            words.append(word)
    
    # Remove duplicates while preserving order
    return list(dict.fromkeys(words))

import os

def load_safe_words(swear_words: set = set()) -> set:
    """Load safe words from SCOWL while explicitly excluding swear words and their common variants."""
    safe_words = set(COMMON_SAFE_WORDS)
    swear_words = swear_words or set()
    
    filename = "english-words.60"  # Make sure this file exists

    if not os.path.exists(filename):
        print(f"Warning: Safe words list '{filename}' not found! Falling back to COMMON_SAFE_WORDS only.")
        return safe_words
    
    try:
        with open(filename, "r", encoding="ISO-8859-1") as f:  # Use ISO-8859-1 to handle special characters
            for line in f:
                word = line.strip().lower()
                if not word or word in swear_words:
                    continue
                
                is_swear_variant = any(word.startswith(swear) and len(word) - len(swear) <= 3 for swear in swear_words)
                if not is_swear_variant:
                    safe_words.add(word)
    
    except Exception as e:
        print(f"Error loading safe words: {e}")
    
    return safe_words

# ==================== MAIN FILTER CLASS ====================
class SwearFilter:
    def __init__(self, swear_words: set, strict_mode: bool = False):
        self.swear_words = set(word.lower().strip() for word in swear_words)
        self.safe_words = set()  # Already loaded externally if needed
        self.strict_mode = strict_mode
        self.message_cache = {}
        self.cache_max_size = 1000
        self.cache_lock = asyncio.Lock()
   
    def _expand_variants(self, word: str, limit: int = 10000) -> Set[str]:
        from itertools import product

        options = []
        for c in word:
            variants = COMBINED_SUBSTITUTIONS.get(c.lower(), [c.lower()])
            options.append(variants)

        result = set()
        for combo in product(*options):
            result.add(''.join(combo))
            if len(result) >= limit:
                break
        return result

    def _normalize_unicode(self, text: str) -> str:
        normalized = []
        for char in unicodedata.normalize('NFKD', text):
            if not unicodedata.combining(char):
                # Convert to ASCII and lowercase
                c = char.encode('ascii', 'ignore').decode('ascii').lower()
                normalized.append(c if c else '')
        return ''.join(normalized)

    def _simplify_repeats(self, text: str) -> str:
        """Reduce repeated characters to 1 (aaa -> a)"""
        return self.repeat_pattern.sub(r'\1', text)

    def _compile_patterns(self) -> Dict[str, re.Pattern]:
        patterns = {}
        for word in self.swear_words:
            # Base pattern for standard spelling
            base_pattern = re.escape(word)
            
            # Advanced pattern covering:
            # 1. Homoglyphs (ğ’®ğ’½ğ’¾ğ“‰)
            # 2. Leet speak (5h1t)
            # 3. Repeated letters (shiiiiit)
            # 4. Special chars/spaces (s h i t)
            pattern = (
                r'(?:\W|^)(?:'
                rf'[{re.escape(word)}]{{2,}}|'  # Repeated chars
                rf'{base_pattern}|'  # Exact match
                rf'[^\w\s]*{base_pattern}[^\w\s]*|'  # With special chars
                rf'(?:{"|".join(re.escape(c) for c in word)}){{3,}}'  # Split chars
                r')(?:\W|$)'
            )
            patterns[word] = re.compile(pattern, re.IGNORECASE)
        return patterns
    def _compile_all_patterns(self) -> Dict[str, re.Pattern]:
        """Generate regex patterns for all swear words (with variants)"""
        patterns = {}
        for word in self.swear_words:
            pattern_parts = []
            for char in word:
                if char in COMBINED_SUBSTITUTIONS:
                    variants = COMBINED_SUBSTITUTIONS[char]
                    escaped = [re.escape(v) for v in variants]
                    pattern_parts.append(f'[{"".join(escaped)}]')
                else:
                    pattern_parts.append(re.escape(char))
            
            base_pattern = r'[\W_]*'.join(pattern_parts)
            full_pattern = rf'(?<!\w){base_pattern}(?!\w)'
            try:
                patterns[word] = re.compile(full_pattern, re.IGNORECASE)
            except re.error:
                patterns[word] = re.compile(re.escape(word), re.IGNORECASE)
        
        return patterns

    async def _get_cached_result(self, message: str):
        return self.message_cache.get(message)

    async def _cache_message_result(self, message: str, result: bool):
        async with self.cache_lock:
            if len(self.message_cache) >= self.cache_max_size:
                self.message_cache.pop(next(iter(self.message_cache)))
            self.message_cache[message] = result

    def _check_context(self, message: str, word: str) -> bool:
        return False  # Hook for context-aware rules if needed

    def _normalize_text(self, text: str) -> str:
        """Ultimate text normalizer that handles all edge cases while preserving word boundaries"""
        if not text:
            return ""

        # Step 1: Remove hidden characters
        text = remove_hidden_chars(text)
        
        # Step 2: Standard Unicode normalization
        text = unicodedata.normalize('NFKC', text).lower()
        
        # Step 3: Process text word by word to maintain word boundaries
        result = []
        words = re.findall(r'\b[\w\']+\b', text)
        remaining_text = text
        
        for word in words:
            # Find the word in the original text
            word_pos = remaining_text.find(word)
            if word_pos != -1:
                # Add any non-word characters before this word
                result.append(remaining_text[:word_pos])
                remaining_text = remaining_text[word_pos + len(word):]
                
                # Process each character in the word
                normalized_word = []
                for char in word:
                    # Use a consistent normalization map
                    if char in NORMALIZATION_MAP:
                        normalized_word.append(NORMALIZATION_MAP[char])
                    else:
                        normalized_word.append(char)
                
                # Add the normalized word
                norm_word = ''.join(normalized_word)
                # Simplify repeated characters (aaa -> a)
                norm_word = self._simplify_repeats(norm_word)
                result.append(norm_word)
            
        # Add any remaining text
        if remaining_text:
            result.append(remaining_text)
            
        return ''.join(result)

    def _simplify_repeats(self, text: str) -> str:
        """Reduce repeated characters to 1 (aaa -> a)"""
        return self.repeat_pattern.sub(r'\1', text)    
    def debug_normalization(self, text: str) -> dict:
        """Debug helper to show exactly which substitutions are being made"""
        result = {}
        for i, char in enumerate(text):
            if char in NORMALIZATION_MAP and NORMALIZATION_MAP[char] != char:
                result[f"Position {i}"] = {
                    "Original": char,
                    "Normalized": NORMALIZATION_MAP[char],
                    "Unicode point": f"U+{ord(char):04X}"
                }
        return result    
    
    def _check_context(self, message: str, word: str) -> bool:
        """Check if word is in a whitelisted context (e.g., 'classic' vs 'ass')"""
        if word not in CONTEXT_WHITELIST:
            return False
        
        rules = CONTEXT_WHITELIST[word]
        start_time = time.time()
        
        for pattern in rules['patterns']:
            if re.search(pattern, message, re.IGNORECASE):
                return True
            if time.time() - start_time > rules.get('timeout', 2.0):
                break
        
        return False

    def _check_suffix_variations(self, word: str) -> bool:
        """Check for suffixed swears (e.g., 'fucker')"""
        for suffix, rules in SUFFIX_RULES.items():
            if (len(word) >= rules['min_length'] and 
                word.endswith(suffix) and
                word[:-len(suffix)] in self.swear_words and
                word not in rules['exceptions']):
                return True
        
        # Check common prefixes (e.g., 'unfuck')
        common_prefixes = ['re', 'un', 'de', 'in', 'pre', 'pro']
        for prefix in common_prefixes:
            if word.startswith(prefix) and word[len(prefix):] in self.swear_words:
                return True
        
        return False

    def _check_short_swears(self, text: str) -> bool:
        """Detect short swears (e.g., 'wtf', 'fku')"""
        if len(text) <= 3 and text.lower() in SHORT_SWEARS:
            return True
        
        normalized = re.sub(r'[1378245609@#$+*]', '', text.lower())
        return len(normalized) <= 3 and normalized in SHORT_SWEARS

    def _is_english(self, text: str) -> bool:
        """Check if text is English (to reduce false positives)"""
        try:
            return detect(text) == 'en'
        except LangDetectException:
            return True
            
    async def contains_swear_word(self, message: str) -> bool:
        if cached := await self._get_cached_result(message):
            return cached

        if not message or not self.swear_words:
            await self._cache_message_result(message, False)
            return False

        # === RAW token expansion
        words_raw = re.findall(r'\S+', message)
        for word in words_raw:
            variants = expand_all_normalizations(word)
            if any(v in self.swear_words for v in variants):
                await self._cache_message_result(message, True)
                return True

        # === Full normalization
        normalized = preprocess_text_for_filtering(message)
        words_in_message = re.findall(r'\b[\w\']+\b', normalized)

        # === Safe word bypass
        for word in words_in_message:
            if word in self.safe_words and word not in self.swear_words:
                await self._cache_message_result(message, False)
                return False

        # === Direct match
        for word in words_in_message:
            if word in self.swear_words:
                if not self._check_context(message, word):
                    await self._cache_message_result(message, True)
                    return True

        # === Root + suffix match
        for word in words_in_message:
            for swear in self.swear_words:
                if len(swear) < 3: continue
                for i in range(len(word) - len(swear) + 1):
                    segment = word[i:i + len(swear)]
                    variants = expand_all_normalizations(segment)
                    if swear in variants:
                        suffix_len = len(word) - (i + len(swear))
                        if suffix_len <= 3 and not self._check_context(message, word):
                            await self._cache_message_result(message, True)
                            return True

        # === Short-form swears
        if (len(words_in_message) == 1 and
            len(words_in_message[0]) <= 3 and
            words_in_message[0] in SHORT_SWEARS):
            await self._cache_message_result(message, True)
            return True

        # === Phonetic fallback
        phonetic = simple_metaphone(normalized)
        for swear in self.swear_words:
            if simple_metaphone(swear) in phonetic:
                if not self._check_context(message, swear):
                    await self._cache_message_result(message, True)
                    return True

        await self._cache_message_result(message, False)
        return False
    async def _update_cache(self, key: str, value: bool):
        """Thread-safe cache update"""
        async with self.cache_lock:
            if len(self.message_cache) >= self.cache_max_size:
                self.message_cache.pop(next(iter(self.message_cache)))
            self.message_cache[key] = value

    def _normalize_text(self, text: str) -> str:
        """Optimized text normalization"""
        if not text:
            return ""

        # Combine multiple normalization steps
        text = remove_hidden_chars(text)
        text = unicodedata.normalize('NFKC', text).lower()
        text = self._simplify_repeats(text)
        
        # Fast character-by-character normalization
        result = []
        for char in text:
            result.append(NORMALIZATION_MAP.get(char, char))
        return ''.join(result)
    async def test_filter(self, variations: List[str]) -> Dict[str, bool]:
        """Test the filter against a list of variations"""
        return {var: await self.contains_swear_word(var) for var in variations} 
    
if __name__ == "__main__":
    test_words = [
        "fuck", "f@ck", "Æ’Ã¼Â¢k", "f*u*c*k", "f.u.c.k", "f u c k", "ğŸ…µğŸ†„ğŸ…²ğŸ…º",
        "shit", "$hit", "sh!t", "s#it", "s.h.i.t", "s h i t",
        "damn", "d@mn", "d@mn!", "D4MN", "d4mn"
    ]

    swear_words = ["fuck", "shit", "damn"]
    sf = SwearFilter(swear_words)

    async def run_tests():
        for msg in test_words:
            result = await sf.contains_swear_word(msg)
            print(f"{msg:15} => {'ğŸš« BLOCKED' if result else 'âœ… ALLOWED'}")

    asyncio.run(run_tests())
